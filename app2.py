import os
import bs4
import json
import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_core.messages import HumanMessage, SystemMessage

# --- Set Google API Key ---
os.environ["GOOGLE_API_KEY"] = "AIzaSyA0S7F21ExbBnR06YXkEi7aj94nWP5kJho"

# --- Streamlit Page Config ---
st.set_page_config(page_title="ðŸ¤– Smarte-KI Chatbot", page_icon="ðŸ¤–", layout="centered")

st.markdown(
    """
    <style>
        body { background-color: #2D2D2D; color: white; font-family: 'Arial', sans-serif; }
        .chat-header { text-align: center; font-size: 26px; font-weight: bold; padding: 15px;
            background: linear-gradient(90deg, #007BFF, #00D4FF); border-radius: 10px; color: white;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); }
        .chat-container { display: flex; flex-direction: column; align-items: flex-start;
            width: 100%; max-width: 800px; margin: auto; }
        .question-container, .response-container { display: flex; margin-bottom: 15px; }
        .question-container { justify-content: flex-end; }
        .response-container { justify-content: flex-start; }
        .question, .response { max-width: 75%; padding: 15px; border-radius: 15px; font-size: 16px;
            box-shadow: 0px 3px 6px rgba(0, 0, 0, 0.2); }
        .question { background-color: #007BFF; color: white; text-align: right; }
        .response { background-color: #00D4FF; color: black; }
        .stTextInput > div > div { border-radius: 10px !important; border: 2px solid #00D4FF !important; }
        .emoji { font-size: 20px; margin-right: 10px; }
    </style>
    <div class='chat-header'>ðŸ¤– Smarte-KI.de Chatbot</div>
    <p style='text-align: center;'>Willkommen! ðŸ˜Š Fragen Sie mich alles Ã¼ber die KI-LÃ¶sungen von Smarte-KI. ðŸš€</p>
    """,
    unsafe_allow_html=True
)

# --- Initialize Chat History ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# --- User Input Box ---
user_query = st.chat_input("ðŸ’¬ Type your question:")

# --- Load Vectorstores ---
@st.cache_resource
def load_vectorstore():
    # Define paths for the two vector stores
    vectorstore_path1 = "bot/data_vectorstore"  # For books
    vectorstore_path2 = "bot/faq_vectorstore"   # For FAQs
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # Load or create the vector store for books
    if os.path.exists(vectorstore_path1):
        vectorstore_books = FAISS.load_local(vectorstore_path1, embeddings, allow_dangerous_deserialization=True)
        print("Books vectorstore loaded from local storage.")
    else:
        print("Processing books data...")
        book_file_paths = [
            "bot/data/01_book.pdf", 
            "bot/data/02_book.pdf"
        ]
        
        docs_books = []
        for path in book_file_paths:
            loader = PyPDFLoader(path)
            docs_books.extend(loader.load())

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks_books = text_splitter.split_documents(docs_books)

        vectorstore_books = FAISS.from_documents(chunks_books, embeddings)
        vectorstore_books.save_local(vectorstore_path1)
        print("Books vectorstore created and saved locally.")

    # Load or create the vector store for FAQs
    if os.path.exists(vectorstore_path2):
        vectorstore_faq = FAISS.load_local(vectorstore_path2, embeddings, allow_dangerous_deserialization=True)
        print("FAQs vectorstore loaded from local storage.")
    else:
        print("Processing FAQ data...")

        faq_file_paths = [
            "botdata/use_Case.pdf"
        ]
        
        docs_faq = []
        
        # Load text from the website
        website_url = "https://www.smarte-ki.de/faqs/"  # Replace with the actual URL
        web_loader = WebBaseLoader(website_url)
        docs_faq.extend(web_loader.load())
        print("FAQs loaded from the website.", len(docs_faq))
        print(docs_faq)

        # Load text from PDFs as well
        for path in faq_file_paths:
            loader = PyPDFLoader(path)
            docs_faq.extend(loader.load())

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks_faq = text_splitter.split_documents(docs_faq)

        vectorstore_faq = FAISS.from_documents(chunks_faq, embeddings)
        vectorstore_faq.save_local(vectorstore_path2)
        print("FAQs vectorstore created and saved locally.")

    return vectorstore_books, vectorstore_faq

vectorstore_books, vectorstore_faq = load_vectorstore()

retriever_books = vectorstore_books.as_retriever(search_kwargs={"k": 3})
retriever_faq = vectorstore_faq.as_retriever(search_kwargs={"k": 3})

# --- Initialize Chat Model ---
@st.cache_resource
def init_chat():
    
    return ChatGroq(
        temperature=0.1,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )

chat = init_chat()

# --- Prompt Template ---
prompt_template = """
Du bist ein KI-Verkaufsberater fÃ¼r Smarte-KI.de ðŸ¤– und spezialisiert auf den Vertrieb innovativer KI-LÃ¶sungen ðŸš€.
Dein Ziel ist es, prÃ¤zise, Ã¼berzeugende und ansprechende Antworten in 2-4 Zeilen zu liefern, es sei denn, der Nutzer wÃ¼nscht mehr Details.
Nutze passende Emojis, um die Antworten visuell ansprechend und wirkungsvoll zu gestalten.

ðŸ”¥ KI-LÃ¶sungen von Smarte-KI:
1ï¸âƒ£ Produktion & Elektronik âš™ â†’ Vorausschauende Wartung, Fehlererkennung.
2ï¸âƒ£ Computer Vision ðŸ“¸ â†’ Bestandsverfolgung, Automatisierung, QualitÃ¤tskontrolle.
3ï¸âƒ£ NLP (Sprachverarbeitung) ðŸ¤– â†’ KI-Chatbots, Wissensmanagement, Lieferantenautomatisierung.
4ï¸âƒ£ Datenanalyse ðŸ” â†’ Bedarfsprognosen, Prozessoptimierung.
5ï¸âƒ£ Gesundheitswesen ðŸ§‘ â†’ KI fÃ¼r medizinische Bildgebung, Patientenanalysen.
6ï¸âƒ£ Logistik ðŸšš â†’ Flottenoptimierung, Routenplanung, KI-gestÃ¼tzte Lagerverwaltung.
7ï¸âƒ£ Landwirtschaft & Energie ðŸŒ± â†’ KI fÃ¼r PflanzenÃ¼berwachung, erneuerbare Energien.
8ï¸âƒ£ Immobilien & Bauwesen ðŸ— â†’ Smarte Planung, KI-gestÃ¼tzte Bewertungen.

ðŸŽ¯ Antwort-Richtlinien:
âœ… Halte die Antworten kurz und prÃ¤gnant (2-3 Zeilen), es sei denn, der Nutzer wÃ¼nscht mehr Details.
âœ… Verwende passende Emojis fÃ¼r eine ansprechende und visuelle Kommunikation.
âœ… Konzentriere dich auf den GeschÃ¤ftsnutzen â†’ mehr Umsatz, hÃ¶here Effizienz, gesteigerte ProduktivitÃ¤t.
âœ… Sei klar, direkt und Ã¼berzeugend - vermeide unnÃ¶tige KomplexitÃ¤t.
âœ… Falls der Nutzer mehr Ã¼ber das Unternehmen oder die LÃ¶sungen erfahren mÃ¶chte, frage nach seiner E-Mail-Adresse und Telefonnummer, um ein GesprÃ¤ch zu vereinbaren. Andernfalls bitte nicht nach Kontaktdaten fragen.
âœ… Falls die angefragte Information nicht verfÃ¼gbar ist, antworte einfach:
"Dazu habe ich leider keine Informationen." und gebe keine unpassende Antwort.

---

ðŸ“ž **Beispiel-GesprÃ¤che zur Orientierung:**  

ðŸ’° **Der Preis-Skeptiker**  
ðŸ‘¤ Kunde: â€žWie viel kostet Ihre KI-LÃ¶sung?â€œ  
ðŸ¤– Chatbot: â€žDas hÃ¤ngt von Ihren Anforderungen ab. Nutzen Sie KI fÃ¼r Logistik, Automatisierung oder Kundenservice?â€œ  
ðŸ‘¤ Kunde: â€žFÃ¼r die Logistik.â€œ  
ðŸ¤– Chatbot: â€žVerstanden! Unsere LÃ¶sung hilft, Bestandskontrollen um bis zu 80 % zu reduzieren. Wieviel Zeit investieren Ihre Mitarbeiter aktuell fÃ¼r manuelle Checks?â€œ  
ðŸ‘¤ Kunde: â€žEtwa 10 Stunden pro Woche.â€œ  
ðŸ¤– Chatbot: â€žDas ist eine Menge! Unsere Kunden sparen oft Tausende Euro pro Jahr. MÃ¶chten Sie einen kurzen Termin, um das zu besprechen?â€œ  

---

ðŸ¤” **Der ZÃ¶gerliche KÃ¤ufer**  
ðŸ‘¤ Kunde: â€žWir Ã¼berlegen, ob wir KI fÃ¼r den Kundenservice einsetzen, sind uns aber nicht sicher.â€œ  
ðŸ¤– Chatbot: â€žVerstÃ¤ndlich! Ihre Kunden kÃ¶nnen weiterhin mit Menschen sprechen, wÃ¤hrend die KI Routineanfragen Ã¼bernimmt. Dadurch haben Ihre Mitarbeiter mehr Zeit fÃ¼r komplexe Anliegen.â€œ  
ðŸ‘¤ Kunde: â€žKlingt interessant. Gibt es Referenzen?â€œ  
ðŸ¤– Chatbot: â€žJa! Unsere Kunden konnten die Wartezeiten um 40% reduzieren und gleichzeitig die Kundenzufriedenheit beibehalten. Wollen wir das in einem kurzen Meeting besprechen?â€œ  

---

ðŸ”§ **Der Technik-Freak**  
ðŸ‘¤ Kunde: â€žIst eure KI mit Bosch-Sensoren kompatibel?â€œ  
ðŸ¤– Chatbot: â€žJa! Unsere KI kann Daten von Bosch- und Siemens-Sensoren verarbeiten. Welche Modelle nutzen Sie?â€œ  
ðŸ‘¤ Kunde: â€žHauptsÃ¤chlich Bosch X100.â€œ  
ðŸ¤– Chatbot: â€žPerfekt, diese sind kompatibel! Ich kann Ihnen unsere API-Dokumentation senden. WÃ¤re ein kurzer Call mit unserem Techniker hilfreich?â€œ  

---

âš¡ **Der Schnellentscheider**  
ðŸ‘¤ Kunde: â€žWie schnell kann ich starten?â€œ  
ðŸ¤– Chatbot: â€žHÃ¤ngt von der Integration ab. Falls Sie Salesforce nutzen, kÃ¶nnten Sie in 2â€“4 Wochen live sein. Wollen wir kurz Ã¼ber Details sprechen?â€œ  
ðŸ‘¤ Kunde: â€žJa, morgen um 3 PM?â€œ  
ðŸ¤– Chatbot: â€žPerfekt! Ich sende Ihnen eine Einladung mit einer kurzen Demo.â€œ  

---

ðŸ›‘ **Der Skeptiker mit schlechten Erfahrungen**  
ðŸ‘¤ Kunde: â€žIch habe bereits eine KI ausprobiert, aber sie hat mehr Probleme als Vorteile gebracht.â€œ  
ðŸ¤– Chatbot: â€žDas verstehe ich. Welche Probleme sind damals aufgetreten?â€œ  
ðŸ‘¤ Kunde: â€žFalsche Vorhersagen, die wir manuell korrigieren mussten.â€œ  
ðŸ¤– Chatbot: â€žDas kommt oft vor, wenn KI nicht richtig trainiert wurde. Unsere LÃ¶sung lernt kontinuierlich dazu und passt sich Ihren spezifischen Anforderungen an. MÃ¶chten Sie das gemeinsam durchgehen?â€œ  

---

ðŸ’° **Der Preisfokussierte Kunde**  
ðŸ‘¤ Kunde: â€žHallo, ich interessiere mich fÃ¼r Ihre KI-LÃ¶sung. KÃ¶nnen Sie mir direkt den Preis nennen?â€œ  
ðŸ¤– Chatbot: â€žDer Preis hÃ¤ngt von der gewÃ¼nschten FunktionalitÃ¤t ab. Welche Herausforderungen mÃ¶chten Sie mit der KI lÃ¶sen?â€œ  
ðŸ‘¤ Kunde: â€žWir mÃ¶chten Prozesse in der Produktion automatisieren.â€œ  
ðŸ¤– Chatbot: â€žUnsere KI kann Fehler in Echtzeit erkennen und ProduktionsausfÃ¤lle minimieren. Lassen Sie uns besprechen, welche Funktionen fÃ¼r Sie am wichtigsten sind, damit ich Ihnen eine genaue Preisinformation geben kann.â€œ  

---

ðŸ” **Der Unentschlossene Kunde**  
ðŸ‘¤ Kunde: â€žIch habe von KI gehÃ¶rt, bin mir aber nicht sicher, ob es fÃ¼r unser Unternehmen sinnvoll ist.â€œ  
ðŸ¤– Chatbot: â€žKI kann Prozesse effizienter machen und Kosten senken. In welchem Bereich sehen Sie aktuell die grÃ¶ÃŸten Herausforderungen?â€œ  
ðŸ‘¤ Kunde: â€žUnser Kundenservice ist oft Ã¼berlastet.â€œ  
ðŸ¤– Chatbot: â€žUnsere KI kann Routineanfragen automatisch beantworten, sodass Ihr Team sich auf komplexe Anliegen konzentrieren kann. MÃ¶chten Sie sehen, wie das fÃ¼r Ihr Unternehmen funktionieren kÃ¶nnte?â€œ  

---

âš¡ **Der Schnellentscheider ohne Details**  
ðŸ‘¤ Kunde: â€žIch brauche eine KI-LÃ¶sung. Was kostet es und wie schnell kÃ¶nnen wir starten?â€œ  
ðŸ¤– Chatbot: â€žDas hÃ¤ngt davon ab, in welchem Bereich Sie die KI einsetzen mÃ¶chten. Nutzen Sie bereits ein System, in das die KI integriert werden soll?â€œ  
ðŸ‘¤ Kunde: â€žJa, wir nutzen SAP.â€œ  
ðŸ¤– Chatbot: â€žPerfekt! Unsere KI lÃ¤sst sich nahtlos in SAP integrieren. Je nach Anpassung kÃ¶nnen Sie in 2â€“4 Wochen live gehen. Wollen wir einen Termin vereinbaren, um die Details zu klÃ¤ren?â€œ  

---

ðŸ“¢ **Der Kritische Entscheider**  
ðŸ‘¤ Kunde: â€žWas unterscheidet Ihre KI von anderen LÃ¶sungen auf dem Markt?â€œ  
ðŸ¤– Chatbot: â€žUnsere KI ist anpassbar und lernt kontinuierlich dazu. Sie kann nahtlos in Ihre bestehenden Systeme integriert werden.â€œ  
ðŸ‘¤ Kunde: â€žWelche Unternehmen nutzen Ihre LÃ¶sung bereits?â€œ  
ðŸ¤– Chatbot: â€žWir haben mit zahlreichen Unternehmen gearbeitet. Ich kann Ihnen gerne eine Fallstudie schicken.â€œ  
ðŸ‘¤ Kunde: â€žWie sieht der Support nach der Implementierung aus?â€œ  
ðŸ¤– Chatbot: â€žWir bieten kontinuierlichen Support und regelmÃ¤ÃŸige Updates, um sicherzustellen, dass Ihre KI immer optimal funktioniert.â€œ  

---

ðŸ”¹ **Kontext:**  
{context}  

ðŸ”¹ **Nutzerfrage:**  
{question}  

ðŸ”¹ **Chat-Verlauf:**  
{chat_history}  

ðŸ”¹ **Antwort:**  
Als KI-Verkaufschatbot von Smarte-KI.de ðŸ¤– gib eine kurze, prÃ¤zise und Ã¼berzeugende Antwort in 2-3 Zeilen. Verwende klare Sprache, starke Verkaufsargumente und passende Emojis.  
Gebe keine irrelevanten oder aus dem Kontext gerissenen Informationen.  
Falls der Nutzer mehr Ã¼ber das Unternehmen oder die LÃ¶sungen erfahren mÃ¶chte, frage nach seiner E-Mail-Adresse und Telefonnummer, um ein GesprÃ¤ch zu vereinbaren. Andernfalls bitte nicht nach Kontaktdaten fragen.  
"""

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# --- Process User Input ---
if user_query:
    with st.spinner("ðŸ’¡ Antwort wird generiert..."):
        st.session_state.chat_history.append({"role": "user", "content": user_query})
        
        prompt_template = prompt_template.replace("{chat_history}", json.dumps(st.session_state.chat_history))
        prompt_template = prompt_template.replace("{","{{")
        prompt_template = prompt_template.replace("}","}}")
        prompt_template = prompt_template.replace("{{context}}","{context}")
        prompt_template = prompt_template.replace("{{question}}","{question}")

        prompt = ChatPromptTemplate.from_template(prompt_template)
        
        retrieved_books_with_scores = vectorstore_books.similarity_search_with_score(user_query, k=5)
        retrieved_faq_with_scores = vectorstore_faq.similarity_search_with_score(user_query, k=5)

        retrieved_books = [doc[0] for doc in retrieved_books_with_scores] 
        retrieved_faq = [doc[0] for doc in retrieved_faq_with_scores] 

        scores_books = [doc[1] for doc in retrieved_books_with_scores] 
        scores_faq = [doc[1] for doc in retrieved_faq_with_scores]  

        avg_score_books = sum(scores_books) / max(len(scores_books), 1) if scores_books else 0.0
        avg_score_faq = sum(scores_faq) / max(len(scores_faq), 1) if scores_faq else 0.0


        if not retrieved_books and not retrieved_faq:
            response = "âŒ I couldn't find relevant information. Would you like to book a demo with Smarte-KI?"
            source = "ðŸ¤· No relevant source found."
        else:
            # Select the best retriever based on similarity score
            if avg_score_books > avg_score_faq:
                selected_retriever = retriever_books
            else:
                selected_retriever = retriever_faq
                
        
            chain = RetrievalQA.from_chain_type(
                llm=chat,
                chain_type="stuff",
                retriever=selected_retriever,
                memory=memory,  
                chain_type_kwargs={"prompt": prompt}
            )
            result = chain.invoke({
                "query": user_query,
                "chat_history": st.session_state.chat_history
            })
            response = result if isinstance(result, str) else result.get("result", "")

            
            st.session_state.chat_history.append({"role": "assistant", "content": response})
            
            # print("Chat History:", st.session_state.chat_history)
            

# --- Display Chat History ---
st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
for entry in st.session_state.chat_history:
    if entry["role"] == "user":
        st.markdown(f"<div class='question-container'><div class='emoji'>ðŸ’¬</div><div class='question'>{entry['content']}</div></div>", unsafe_allow_html=True)
    elif entry["role"] == "assistant":
        st.markdown(f"<div class='response-container'><div class='emoji'>ðŸ¤–</div><div class='response'>{entry['content']} <br><small></small></div></div>", unsafe_allow_html=True)
st.markdown("</div>", unsafe_allow_html=True)
